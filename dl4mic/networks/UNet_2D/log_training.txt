2020-09-02 17:55:01.746195: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-09-02 17:55:01.777382: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz
2020-09-02 17:55:01.778025: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a3e7156d10 executing computations on platform Host. Devices:
2020-09-02 17:55:01.778080: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2020-09-02 17:55:01.778359: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /media/baecker/DONNEES1/programs/fiji-linux64/Fiji.app/lib/linux64:/media/baecker/DONNEES1/programs/fiji-linux64/Fiji.app/mm/linux64
2020-09-02 17:55:01.778370: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)
2020-09-02 17:55:01.778407: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (orion): /proc/driver/nvidia/version does not exist
/home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
WARNING:tensorflow:From /home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

WARNING:tensorflow:From /home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:From /home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

WARNING:tensorflow:From /home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/baecker/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2020-09-02 17:55:13.489851: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2020-09-02 17:55:20.411678: W tensorflow/core/framework/allocator.cc:107] Allocation of 1207959552 exceeds 10% of system memory.
2020-09-02 17:55:20.419675: W tensorflow/core/framework/allocator.cc:107] Allocation of 1207959552 exceeds 10% of system memory.
Namespace(baseDir='../../models', batchSize=4, dataSourcePath='../../unet-data/training/source', dataTargetPath='../../unet-data/training/target', epochs=2, horizontalFlip=True, horizontalShift=10, learningRate=0.001, name='new_unet2D_model', patchSizeXY=512, poolingSteps=2, rotationRange=180, shearRange=10, stepsPerEpoch=0, useDataAugmentation=True, validationFraction=10.0, verticalFlip=True, verticalShift=20, zoomRange=10)
You do not have GPU access.
Expect slow performance.
Tensorflow version is 1.14.0
Creating patches...
number of patches: 200
[31m!! WARNING: Folder already exists and will be overwritten !![0m
Getting class weights...
HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 512, 512, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 512, 512, 64) 640         input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 512, 512, 64) 36928       conv2d_1[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 256, 256, 64) 0           conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 256, 256, 128 73856       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 256, 256, 128 147584      conv2d_3[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 128, 128, 128 0           conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 128, 128, 256 295168      max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 128, 128, 256 590080      conv2d_5[0][0]                   
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, 256, 256, 256 0           conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 256, 256, 128 131200      up_sampling2d_1[0][0]            
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 256, 256, 256 0           conv2d_4[0][0]                   
                                                                 conv2d_7[0][0]                   
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 256, 256, 128 295040      concatenate_1[0][0]              
__________________________________________________________________________________________________
up_sampling2d_2 (UpSampling2D)  (None, 512, 512, 128 0           conv2d_8[0][0]                   
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 512, 512, 64) 32832       up_sampling2d_2[0][0]            
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 512, 512, 128 0           conv2d_2[0][0]                   
                                                                 conv2d_9[0][0]                   
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 512, 512, 64) 73792       concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 512, 512, 2)  1154        conv2d_11[0][0]                  
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 512, 512, 1)  3           conv2d_12[0][0]                  
==================================================================================================
Total params: 1,715,205
Trainable params: 1,715,205
Non-trainable params: 0
__________________________________________________________________________________________________
{'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}
[31m!! WARNING: Model folder already existed and has been removed !![0m
---------------------------- Main training parameters ----------------------------
Number of epochs: 2
Batch size: 4
Number of training dataset: 200
Number of training steps: 45
Number of validation steps: 5
---------------------------- ------------------------ ----------------------------
Epoch 1/2
Found 20 images belonging to 1 classes.
Found 180 images belonging to 1 classes.
Found 20 images belonging to 1 classes.
Found 180 images belonging to 1 classes.
 1/45 [..............................] - ETA: 11:07 - loss: 0.66302020-09-02 17:55:34.418498: W tensorflow/core/framework/allocator.cc:107] Allocation of 1207959552 exceeds 10% of system memory.
2020-09-02 17:55:34.425859: W tensorflow/core/framework/allocator.cc:107] Allocation of 1207959552 exceeds 10% of system memory.
 2/45 [>.............................] - ETA: 10:24 - loss: 3.66552020-09-02 17:55:48.086677: W tensorflow/core/framework/allocator.cc:107] Allocation of 1207959552 exceeds 10% of system memory.
 3/45 [=>............................] - ETA: 9:56 - loss: 2.7529  4/45 [=>............................] - ETA: 9:38 - loss: 2.4612 5/45 [==>...........................] - ETA: 9:21 - loss: 2.1054 6/45 [===>..........................] - ETA: 9:05 - loss: 1.8725 7/45 [===>..........................] - ETA: 8:49 - loss: 1.7059 8/45 [====>.........................] - ETA: 8:35 - loss: 1.5832 9/45 [=====>........................] - ETA: 8:20 - loss: 1.490710/45 [=====>........................] - ETA: 8:06 - loss: 1.416211/45 [======>.......................] - ETA: 7:51 - loss: 1.362112/45 [=======>......................] - ETA: 7:37 - loss: 1.308813/45 [=======>......................] - ETA: 7:23 - loss: 1.262014/45 [========>.....................] - ETA: 7:09 - loss: 1.222615/45 [=========>....................] - ETA: 6:54 - loss: 1.188316/45 [=========>....................] - ETA: 6:41 - loss: 1.158917/45 [==========>...................] - ETA: 6:27 - loss: 1.132318/45 [===========>..................] - ETA: 6:13 - loss: 1.109419/45 [===========>..................] - ETA: 5:59 - loss: 1.088420/45 [============>.................] - ETA: 5:45 - loss: 1.071321/45 [=============>................] - ETA: 5:32 - loss: 1.054022/45 [=============>................] - ETA: 5:18 - loss: 1.036723/45 [==============>...............] - ETA: 5:04 - loss: 1.022324/45 [===============>..............] - ETA: 4:51 - loss: 1.006225/45 [===============>..............] - ETA: 4:37 - loss: 0.994426/45 [================>.............] - ETA: 4:23 - loss: 0.984327/45 [=================>............] - ETA: 4:09 - loss: 0.972528/45 [=================>............] - ETA: 3:56 - loss: 0.964029/45 [==================>...........] - ETA: 3:42 - loss: 0.956130/45 [===================>..........] - ETA: 3:29 - loss: 0.946331/45 [===================>..........] - ETA: 3:15 - loss: 0.938432/45 [====================>.........] - ETA: 3:01 - loss: 0.930433/45 [=====================>........] - ETA: 2:47 - loss: 0.922534/45 [=====================>........] - ETA: 2:33 - loss: 0.915335/45 [======================>.......] - ETA: 2:19 - loss: 0.910436/45 [=======================>......] - ETA: 2:05 - loss: 0.904537/45 [=======================>......] - ETA: 1:51 - loss: 0.898738/45 [========================>.....] - ETA: 1:37 - loss: 0.893939/45 [=========================>....] - ETA: 1:23 - loss: 0.889240/45 [=========================>....] - ETA: 1:09 - loss: 0.884441/45 [==========================>...] - ETA: 56s - loss: 0.8805 42/45 [===========================>..] - ETA: 42s - loss: 0.875943/45 [===========================>..] - ETA: 28s - loss: 0.871844/45 [============================>.] - ETA: 14s - loss: 0.867745/45 [==============================] - 649s 14s/step - loss: 0.8645 - val_loss: 0.6929

Epoch 00001: val_loss improved from inf to 0.69293, saving model to ../../models/new_unet2D_model/weights_best.hdf5
Epoch 2/2
 1/45 [..............................] - ETA: 10:01 - loss: 0.6896 2/45 [>.............................] - ETA: 9:51 - loss: 0.6787  3/45 [=>............................] - ETA: 9:38 - loss: 0.6794 4/45 [=>............................] - ETA: 9:22 - loss: 0.6915 5/45 [==>...........................] - ETA: 9:08 - loss: 0.6993 6/45 [===>..........................] - ETA: 8:53 - loss: 0.6938 7/45 [===>..........................] - ETA: 8:38 - loss: 0.6970 8/45 [====>.........................] - ETA: 8:25 - loss: 0.6956 9/45 [=====>........................] - ETA: 8:13 - loss: 0.701110/45 [=====>........................] - ETA: 8:00 - loss: 0.701211/45 [======>.......................] - ETA: 7:47 - loss: 0.700112/45 [=======>......................] - ETA: 7:35 - loss: 0.695113/45 [=======>......................] - ETA: 7:22 - loss: 0.694814/45 [========>.....................] - ETA: 7:12 - loss: 0.700115/45 [=========>....................] - ETA: 6:58 - loss: 0.699116/45 [=========>....................] - ETA: 6:47 - loss: 0.698917/45 [==========>...................] - ETA: 6:36 - loss: 0.695918/45 [===========>..................] - ETA: 6:24 - loss: 0.694119/45 [===========>..................] - ETA: 6:10 - loss: 0.694920/45 [============>.................] - ETA: 5:55 - loss: 0.695221/45 [=============>................] - ETA: 5:40 - loss: 0.696122/45 [=============>................] - ETA: 5:26 - loss: 0.695423/45 [==============>...............] - ETA: 5:11 - loss: 0.697524/45 [===============>..............] - ETA: 4:56 - loss: 0.697925/45 [===============>..............] - ETA: 4:42 - loss: 0.695626/45 [================>.............] - ETA: 4:27 - loss: 0.694127/45 [=================>............] - ETA: 4:13 - loss: 0.695128/45 [=================>............] - ETA: 3:59 - loss: 0.696829/45 [==================>...........] - ETA: 3:45 - loss: 0.697830/45 [===================>..........] - ETA: 3:30 - loss: 0.697031/45 [===================>..........] - ETA: 3:17 - loss: 0.695232/45 [====================>.........] - ETA: 3:02 - loss: 0.694033/45 [=====================>........] - ETA: 2:48 - loss: 0.692634/45 [=====================>........] - ETA: 2:34 - loss: 0.693335/45 [======================>.......] - ETA: 2:20 - loss: 0.692736/45 [=======================>......] - ETA: 2:06 - loss: 0.694137/45 [=======================>......] - ETA: 1:52 - loss: 0.695438/45 [========================>.....] - ETA: 1:38 - loss: 0.695839/45 [=========================>....] - ETA: 1:24 - loss: 0.695940/45 [=========================>....] - ETA: 1:10 - loss: 0.695941/45 [==========================>...] - ETA: 56s - loss: 0.6961 42/45 [===========================>..] - ETA: 42s - loss: 0.695343/45 [===========================>..] - ETA: 28s - loss: 0.696044/45 [============================>.] - ETA: 14s - loss: 0.696045/45 [==============================] - 649s 14s/step - loss: 0.6951 - val_loss: 0.6919

Epoch 00002: val_loss improved from 0.69293 to 0.69188, saving model to ../../models/new_unet2D_model/weights_best.hdf5
------------------------------------------
Time elapsed: 0.0 hour(s) 21.0 min(s) 39 sec(s)
------------------------------------------
